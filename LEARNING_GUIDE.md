# MiniGPT ä»é›¶æ­å»ºå®Œå…¨æŒ‡å—ï¼šå¾ªåºæ¸è¿›ç‰ˆ

æœ¬æŒ‡å—æ—¨åœ¨å¸®åŠ©ä½ ä»¥**å­¦ä¹ **ä¸ºç›®çš„ï¼Œä»é›¶å¼€å§‹æ­å»ºå±äºä½ è‡ªå·±çš„å¤§è¯­è¨€æ¨¡å‹ â€”â€” **MiniGPT**ã€‚
ï¼ˆæ³¨ï¼šæœ¬é¡¹ç›®å‚è€ƒäº† MiniMind çš„è®¾è®¡æ€è·¯ï¼Œä½†æˆ‘ä»¬å°†ä»å¤´æ„å»ºä¸€ä¸ªå…¨æ–°çš„ MiniGPT æ¨¡å‹ï¼‰

ä¸ºäº†è®©ä½ å­¦å¾—æ›´æ‰å®ï¼Œæˆ‘ä»¬å°†æµç¨‹æ‹†åˆ†ä¸º 10 ä¸ªé‡Œç¨‹ç¢‘ã€‚
**æ¯ä¸ªé‡Œç¨‹ç¢‘å®Œæˆåï¼Œéƒ½é…æœ‰ã€ŒéªŒè¯ç¯èŠ‚ã€å’Œã€Œå½“å‰é¡¹ç›®ç»“æ„å›¾ã€ï¼Œç¡®ä¿ä½ æ¯ä¸€æ­¥éƒ½èµ°å¾—ç¨³å½“ã€‚**

---

## ğŸš€ é˜¶æ®µä¸€ï¼šç¯å¢ƒæ­å»ºä¸åˆ†è¯å™¨ (Tokenizer)

**ç›®æ ‡**ï¼šé…ç½®å¥½ Python ç¯å¢ƒï¼Œå¹¶è®©æœºå™¨èƒ½å¤ŸæŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—ï¼ˆTokenï¼‰ã€‚

### 1.1 å‡†å¤‡å·¥ä½œ
ä½ éœ€è¦å®‰è£… `torch`, `transformers`, `datasets` ç­‰åŸºç¡€åº“ã€‚

### 1.2 æ ¸å¿ƒä»»åŠ¡
1.  å‡†å¤‡ä¸€ä»½çº¯æ–‡æœ¬æ•°æ®ï¼ˆä¾‹å¦‚ `dataset/pretrain_hq.jsonl` ä¸­çš„æ–‡æœ¬ï¼‰ã€‚
2.  ä½¿ç”¨ `tokenizers` åº“è®­ç»ƒä¸€ä¸ª BPE åˆ†è¯å™¨ã€‚
3.  æˆ–è€…ç›´æ¥ä½¿ç”¨é¡¹ç›®æä¾›çš„ `tokenizer.json`ã€‚

### ğŸ§ª éªŒè¯ç¯èŠ‚
åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæµ‹è¯•è„šæœ¬ `test_phase1.py`ï¼š

```python
from transformers import PreTrainedTokenizerFast

# åŠ è½½åˆ†è¯å™¨ (å‡è®¾ä½ æ”¾åœ¨äº† ./model/ ç›®å½•ä¸‹)
tokenizer = PreTrainedTokenizerFast.from_pretrained("./model")

text = "ä½ å¥½ï¼ŒMiniGPTï¼"
input_ids = tokenizer.encode(text)
decoded_text = tokenizer.decode(input_ids)

print(f"åŸæ–‡: {text}")
print(f"Token IDs: {input_ids}")
print(f"è¿˜åŸ: {decoded_text}")

# éªŒè¯ä¸€è‡´æ€§
assert text == decoded_text.replace(" ", "") # æ³¨æ„ï¼šæŸäº›tokenizerä¼šåŠ ç©ºæ ¼ï¼Œè§†æƒ…å†µè°ƒæ•´
print("âœ… é˜¶æ®µä¸€éªŒè¯æˆåŠŸï¼åˆ†è¯å™¨å·¥ä½œæ­£å¸¸ã€‚")
```

### ğŸ“‚ å½“å‰é¡¹ç›®ç»“æ„
```text
minigpt/
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ pretrain_hq.jsonl  # åŸå§‹æ•°æ®
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ tokenizer.json     # æ ¸å¿ƒè¯è¡¨æ–‡ä»¶
â”‚   â””â”€â”€ tokenizer_config.json
â””â”€â”€ test_phase1.py         # åˆšæ‰çš„æµ‹è¯•è„šæœ¬
```

---

## ğŸ—ï¸ é˜¶æ®µäºŒï¼šæ¨¡å‹æ„å»º (Model Architecture)

**ç›®æ ‡**ï¼šæ‰‹å†™ä¸€ä¸ª Transformer æ¨¡å‹ï¼ˆMiniGPTï¼‰ï¼Œè€Œä¸æ˜¯ç›´æ¥ importã€‚

### 2.1 æ ¸å¿ƒä»»åŠ¡
åˆ›å»º `model/model_minigpt.py`ã€‚ä½ éœ€è¦å®ç°ï¼š
1.  `RMSNorm`: å½’ä¸€åŒ–å±‚ã€‚
2.  `RoPE`: æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆè¿™æ˜¯ LLM æ”¯æŒé•¿æ–‡æœ¬çš„å…³é”®ï¼‰ã€‚
3.  `Attention`: è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚
4.  `FeedForward`: å‰é¦ˆç½‘ç»œ (SwiGLU)ã€‚
5.  `MiniGPT`: ç»„åˆä»¥ä¸Šæ¨¡å—ã€‚

### ğŸ§ª éªŒè¯ç¯èŠ‚
åˆ›å»º `test_phase2.py`ï¼Œæ£€æŸ¥æ¨¡å‹èƒ½ä¸èƒ½è·‘é€šä¸€æ¬¡â€œå‰å‘ä¼ æ’­â€ï¼š

```python
import torch
from model.model_minigpt import MiniGPT, MiniGPTConfig

# 1. åˆå§‹åŒ–é…ç½® (ä½¿ç”¨æå°é…ç½®ä»¥å¿«é€Ÿæµ‹è¯•)
config = MiniGPTConfig(
    vocab_size=6400,
    hidden_size=256,   # å°ä¸€ç‚¹æ–¹ä¾¿CPUæµ‹
    num_hidden_layers=2,
    num_attention_heads=4,
    max_position_embeddings=512
)

# 2. å®ä¾‹åŒ–æ¨¡å‹
model = MiniGPT(config)
print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")

# 3. æ„é€ è™šæ‹Ÿè¾“å…¥ (Batch=2, SeqLen=10)
dummy_input = torch.randint(0, 6400, (2, 10))

# 4. å‰å‘ä¼ æ’­
output = model(dummy_input)

# 5. æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
# æœŸæœ›è¾“å‡º: [Batch, SeqLen, VocabSize]
expected_shape = (2, 10, 6400)
assert output.logits.shape == expected_shape
print(f"è¾“å‡ºå½¢çŠ¶: {output.logits.shape}")
print("âœ… é˜¶æ®µäºŒéªŒè¯æˆåŠŸï¼æ¨¡å‹ç»“æ„æ­å»ºå®Œæ¯•ï¼Œè¾“å…¥è¾“å‡ºå¯¹é½ã€‚")
```

### ğŸ“‚ å½“å‰é¡¹ç›®ç»“æ„
```text
minigpt/
â”œâ”€â”€ dataset/ ...
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_minigpt.py  # <--- æ–°å¢æ ¸å¿ƒä»£ç 
â”‚   â””â”€â”€ tokenizer...
â”œâ”€â”€ test_phase1.py
â””â”€â”€ test_phase2.py         # <--- æ–°å¢æµ‹è¯•
```

---

## ğŸ“š é˜¶æ®µä¸‰ï¼šæ•°æ®ç®¡é“ (Dataset Pipeline)

**ç›®æ ‡**ï¼šæŠŠåŸå§‹æ–‡æœ¬å¤„ç†æˆæ¨¡å‹èƒ½åƒçš„ `Tensor`ï¼Œç‰¹åˆ«æ˜¯è¦ææ‡‚ **Mask**ã€‚

### 3.1 æ ¸å¿ƒä»»åŠ¡
åˆ›å»º `dataset/lm_dataset.py`ã€‚
1.  **PretrainDataset**: ç®€å•çš„æ»‘çª—æˆªæ–­ã€‚è¾“å…¥ `x` æ˜¯ `[0:-1]`, æ ‡ç­¾ `y` æ˜¯ `[1:]`ã€‚
2.  **SFTDataset**: **(é‡éš¾ç‚¹)** å¤„ç†å¯¹è¯æ ¼å¼ã€‚
    *   æ„é€  Input: `<|im_start|>user\nä½ å¥½<|im_end|>\n<|im_start|>assistant\næˆ‘æ˜¯MiniGPT<|im_end|>`
    *   æ„é€  Mask: åªæœ‰â€œæˆ‘æ˜¯MiniGPTâ€è¿™éƒ¨åˆ†çš„ loss åº”è¯¥è¢«è®¡ç®—ï¼Œuser çš„æé—®éƒ¨åˆ† loss mask è®¾ä¸º 0ã€‚

### ğŸ§ª éªŒè¯ç¯èŠ‚
åˆ›å»º `test_phase3.py`ï¼Œè‚‰çœ¼æ£€æŸ¥ Mask å¯¹ä¸å¯¹ï¼š

```python
from transformers import PreTrainedTokenizerFast
from dataset.lm_dataset import SFTDataset

tokenizer = PreTrainedTokenizerFast.from_pretrained("./model")

# æ¨¡æ‹Ÿä¸€ä¸ª SFT æ•°æ®æ–‡ä»¶
import json
with open("test_sft.jsonl", "w", encoding="utf-8") as f:
    data = {
        "conversations": [
            {"role": "user", "content": "A"},
            {"role": "assistant", "content": "B"}
        ]
    }
    f.write(json.dumps(data, ensure_ascii=False))

# åŠ è½½æ•°æ®é›†
ds = SFTDataset("test_sft.jsonl", tokenizer, max_length=64)
x, y, mask = ds[0]

print("Input:", tokenizer.decode(x))
print("Mask :", mask.tolist())

# ç®€å•éªŒè¯: useréƒ¨åˆ†(A)çš„maskåº”è¯¥æ˜¯0, assistantéƒ¨åˆ†(B)çš„maskåº”è¯¥æ˜¯1
# æ³¨æ„: ä¸åŒtokenizerå¤„ç†ç‰¹æ®Šå­—ç¬¦æ–¹å¼ä¸åŒï¼Œå»ºè®®æ‰“å°å‡ºæ¥è‚‰çœ¼ç¡®è®¤ '1' è¦†ç›–äº†å›ç­”éƒ¨åˆ†
print("âœ… é˜¶æ®µä¸‰éªŒè¯å®Œæˆï¼è¯·äººå·¥ç¡®è®¤ Mask æ˜¯å¦è¦†ç›–äº†å›ç­”éƒ¨åˆ†ã€‚")
```

### ğŸ“‚ å½“å‰é¡¹ç›®ç»“æ„
```text
minigpt/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ pretrain_hq.jsonl
â”‚   â””â”€â”€ lm_dataset.py      # <--- æ–°å¢æ•°æ®å¤„ç†é€»è¾‘
â”œâ”€â”€ model/ ...
â”œâ”€â”€ test_sft.jsonl         # ä¸´æ—¶æµ‹è¯•æ–‡ä»¶
â””â”€â”€ test_phase3.py         # <--- æ–°å¢æµ‹è¯•
```

---

## ğŸ‹ï¸ é˜¶æ®µå››ï¼šé¢„è®­ç»ƒå¾ªç¯ (Pretraining Loop)

**ç›®æ ‡**ï¼šå†™å‡ºè®­ç»ƒå¾ªç¯ï¼Œè®© Loss åŠ¨èµ·æ¥ã€‚

### 4.1 æ ¸å¿ƒä»»åŠ¡
åˆ›å»º `trainer/train_pretrain.py`ã€‚
1.  åŠ è½½ Model å’Œ Datasetã€‚
2.  åˆå§‹åŒ– Optimizer (AdamW)ã€‚
3.  ç¼–å†™ Loop: `Forward` -> `Loss` -> `Backward` -> `Step`ã€‚
4.  ä¿å­˜æ¨¡å‹æƒé‡ (`.pth` æˆ– `.safetensors`)ã€‚

### ğŸ§ª éªŒè¯ç¯èŠ‚
ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬ï¼Œä½†å‚æ•°è®¾å¾—å¾ˆå°ï¼Œåªè·‘å‡ æ­¥ï¼š

```bash
# å‘½ä»¤è¡Œæµ‹è¯•
python trainer/train_pretrain.py --epochs 1 --batch_size 2 --save_dir ./out_test
```

**æ£€æŸ¥ç‚¹**ï¼š
1.  ç»ˆç«¯æ˜¯å¦æ‰“å°å‡º Loss (ä¾‹å¦‚ `loss: 8.5432`)ï¼Ÿ
2.  Loss æ˜¯å¦ä¸æ˜¯ `NaN`ï¼Ÿ
3.  `./out_test` ç›®å½•ä¸‹æ˜¯å¦ç”Ÿæˆäº† `.pth` æ–‡ä»¶ï¼Ÿ

### ğŸ“‚ å½“å‰é¡¹ç›®ç»“æ„
```text
minigpt/
â”œâ”€â”€ dataset/ ...
â”œâ”€â”€ model/ ...
â”œâ”€â”€ trainer/
â”‚   â””â”€â”€ train_pretrain.py  # <--- æ–°å¢è®­ç»ƒè„šæœ¬
â”œâ”€â”€ out_test/              # <--- ç”Ÿæˆçš„æƒé‡ç›®å½•
â”‚   â””â”€â”€ pretrain_xxx.pth
â””â”€â”€ ...
```

---

## ğŸ—£ï¸ é˜¶æ®µäº”ï¼šç›‘ç£å¾®è°ƒ (SFT)

**ç›®æ ‡**ï¼šè®©æ¨¡å‹å­¦ä¼šå¯¹è¯æ ¼å¼ï¼Œä¸å†èƒ¡è¨€ä¹±è¯­ã€‚

### 5.1 æ ¸å¿ƒä»»åŠ¡
åˆ›å»º `trainer/train_full_sft.py`ã€‚
*   é€»è¾‘ä¸é¢„è®­ç»ƒå‡ ä¹ä¸€æ ·ï¼Œä½†åŠ è½½çš„æ˜¯ `SFTDataset`ã€‚
*   éœ€è¦åŠ è½½**é˜¶æ®µå››**è®­ç»ƒå¥½çš„é¢„è®­ç»ƒæƒé‡ä½œä¸ºèµ·ç‚¹ (Init from Pretrain)ã€‚

### ğŸ§ª éªŒè¯ç¯èŠ‚
åŒæ ·è¿è¡Œä¸€ä¸ªå°æµ‹è¯•ï¼š
```bash
python trainer/train_full_sft.py --epochs 1 --batch_size 2 --save_dir ./out_sft_test
```
ç¡®è®¤ Loss ä¸‹é™ï¼Œä¸”ä¿å­˜äº†æ–°çš„æƒé‡ã€‚

### ğŸ“‚ å½“å‰é¡¹ç›®ç»“æ„
```text
minigpt/
â”œâ”€â”€ dataset/ ...
â”œâ”€â”€ model/ ...
â”œâ”€â”€ trainer/
â”‚   â”œâ”€â”€ train_pretrain.py
â”‚   â””â”€â”€ train_full_sft.py  # <--- æ–°å¢ SFT è„šæœ¬
â”œâ”€â”€ out_sft_test/          # <--- SFT æƒé‡
â””â”€â”€ ...
```

---

## ğŸ¤– é˜¶æ®µå…­ï¼šæ¨ç†ä¸å¯¹è¯ (Inference)

**ç›®æ ‡**ï¼šè§è¯å¥‡è¿¹çš„æ—¶åˆ»ï¼Œå’Œä½ çš„æ¨¡å‹èŠå¤©ã€‚

### 6.1 æ ¸å¿ƒä»»åŠ¡
åˆ›å»º `scripts/web_demo.py` æˆ–ç®€å•çš„ `chat.py`ã€‚
1.  åŠ è½½ SFT åçš„æƒé‡ã€‚
2.  å®ç° `generate` å‡½æ•°ï¼ˆå¦‚æœæ˜¯æ‰‹å†™çš„ï¼‰æˆ–è°ƒç”¨ `model.generate`ã€‚
3.  å¤„ç† `Input` -> `Tokenizer` -> `Model` -> `Tokenizer` -> `Output` çš„æµå‘ã€‚

### ğŸ§ª éªŒè¯ç¯èŠ‚
åˆ›å»º `test_chat.py`ï¼š

```python
import torch
from transformers import PreTrainedTokenizerFast
from model.model_minigpt import MiniGPT, MiniGPTConfig

# 1. åŠ è½½é…ç½®å’Œæ¨¡å‹
tokenizer = PreTrainedTokenizerFast.from_pretrained("./model")
model = MiniGPT(MiniGPTConfig(...)) # å¡«å…¥ä½ çš„é…ç½®
# åŠ è½½ä½ è®­ç»ƒå¥½çš„ SFT æƒé‡
state_dict = torch.load("./out_sft_test/xxx.pth", map_location='cpu')
model.load_state_dict(state_dict, strict=False)
model.eval()

# 2. å¯¹è¯
prompt = "ä½ å¥½"
messages = [{"role": "user", "content": prompt}]
input_str = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
input_ids = tokenizer(input_str, return_tensors='pt').input_ids

with torch.no_grad():
    outputs = model.generate(input_ids, max_new_tokens=50)
    
response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)
print(f"User: {prompt}")
print(f"MiniGPT: {response}")

print("âœ… é˜¶æ®µå…­éªŒè¯å®Œæˆï¼å¦‚æœå›å¤é€šé¡ºï¼Œæ­å–œä½ å¤ç°æˆåŠŸï¼")
```

### ğŸ“‚ å½“å‰é¡¹ç›®ç»“æ„
```text
minigpt/
â”œâ”€â”€ dataset/         # æ•°æ®å¤„ç†
â”œâ”€â”€ model/           # æ¨¡å‹å®šä¹‰ & Tokenizer
â”œâ”€â”€ trainer/         # è®­ç»ƒè„šæœ¬ (Pretrain, SFT)
â”œâ”€â”€ scripts/         # æ¨ç† & Demo
â”œâ”€â”€ out/             # å­˜æ”¾è®­ç»ƒå¥½çš„æƒé‡
â””â”€â”€ tests/           # (æ¨è) å­˜æ”¾æ‰€æœ‰çš„æµ‹è¯•è„šæœ¬
```

---

## ğŸï¸ é˜¶æ®µä¸ƒï¼šLoRA å¾®è°ƒ (Low-Rank Adaptation)

**ç›®æ ‡**ï¼šä»¥æå°çš„æ˜¾å­˜ä»£ä»·ï¼ˆå‡ MBå‚æ•°ï¼‰å¾®è°ƒå¤§æ¨¡å‹ã€‚

### 7.1 æ ¸å¿ƒä»»åŠ¡
1.  **Model (LoRA)**: åˆ›å»º `model/model_lora.py`ã€‚
    *   å®šä¹‰ `LoRA` ç±»ï¼šåŒ…å«ä¸¤ä¸ªä½ç§©çŸ©é˜µ A å’Œ Bã€‚
    *   å®šä¹‰ `apply_lora` å‡½æ•°ï¼šéå†æ¨¡å‹æ‰€æœ‰ `Linear` å±‚ï¼Œå°†å…¶æ›¿æ¢ä¸ºå¸¦ LoRA çš„ç‰ˆæœ¬ã€‚
2.  **Trainer (LoRA)**: åˆ›å»º `trainer/train_lora.py`ã€‚
    *   åŠ è½½é¢„è®­ç»ƒ/SFTæƒé‡ã€‚
    *   è°ƒç”¨ `apply_lora` æ³¨å…¥å‚æ•°ã€‚
    *   **å…³é”®ç‚¹**ï¼šä»…å°† LoRA å‚æ•°è®¾ä¸º `requires_grad=True`ï¼Œå†»ç»“å…¶ä»–å‚æ•°ã€‚

### ğŸ§ª éªŒè¯ç¯èŠ‚
åˆ›å»º `test_phase7.py`ï¼š

```python
import torch
from model.model_minigpt import MiniGPT, MiniGPTConfig
from model.model_lora import apply_lora

model = MiniGPT(MiniGPTConfig(hidden_size=256, num_hidden_layers=2))
print(f"åŸå§‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")

# åº”ç”¨ LoRA
apply_lora(model, rank=8)

# æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"LoRAåå¯è®­ç»ƒå‚æ•°é‡: {trainable_params}")

# ç®€å•å‰å‘ä¼ æ’­
x = torch.randint(0, 100, (1, 10))
y = model(x)
print("âœ… é˜¶æ®µä¸ƒéªŒè¯æˆåŠŸï¼LoRA å·²æ³¨å…¥ä¸”å‰å‘ä¼ æ’­æ­£å¸¸ã€‚")
```

---

## ğŸ§  é˜¶æ®µå…«ï¼šæ¨ç†æ¨¡å‹ (Reasoning / Chain of Thought)

**ç›®æ ‡**ï¼šè®©æ¨¡å‹å­¦ä¼šâ€œæ…¢æ€è€ƒâ€ï¼Œè¾“å‡º `<think>` æ ‡ç­¾ã€‚

### 8.1 æ ¸å¿ƒä»»åŠ¡
åˆ›å»º `trainer/train_reason.py`ã€‚
1.  **Dataset**: å‡†å¤‡åŒ…å«æ€è€ƒè¿‡ç¨‹çš„æ•°æ®ï¼ˆå¦‚ `<think>...è¿‡ç¨‹...</think><answer>...ç»“æœ...</answer>`ï¼‰ã€‚
2.  **Loss Weighting**: è¿™æ˜¯ä¸€ä¸ªå…³é”®æŠ€å·§ã€‚
    *   ä¸ºäº†å¼ºè¿«æ¨¡å‹å­¦ä¼šä½¿ç”¨æ ‡ç­¾ï¼Œåœ¨è®¡ç®— Loss æ—¶ï¼Œç»™ `<think>`, `</think>`, `<answer>`, `</answer>` è¿™äº›ç‰¹æ®Š token **åŠ æƒ**ï¼ˆä¾‹å¦‚ 10å€æƒé‡ï¼‰ã€‚
    *   è¿™èƒ½é˜²æ­¢æ¨¡å‹â€œå·æ‡’â€è·³è¿‡æ€è€ƒè¿‡ç¨‹ã€‚

### ğŸ§ª éªŒè¯ç¯èŠ‚
æŸ¥çœ‹ `train_reason.py` ä¸­çš„ `loss_mask` å¤„ç†é€»è¾‘ã€‚
å¯ä»¥æ‰‹åŠ¨æ„é€ ä¸€ä¸ª Batchï¼Œæ£€æŸ¥ loss_mask ä¸­å¯¹åº” `<think>` çš„ä½ç½®æ˜¯å¦çœŸçš„æ˜¯ 10ã€‚

---

## ğŸ‘® é˜¶æ®µä¹ï¼šRLHF (PPO) â€”â€” äººç±»åå¥½å¯¹é½

**ç›®æ ‡**ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®©æ¨¡å‹æ›´ç¬¦åˆäººç±»ä»·å€¼è§‚ã€‚

### 9.1 æ ¸å¿ƒä»»åŠ¡
åˆ›å»º `trainer/train_ppo.py`ã€‚
1.  **Critic Model**: åŸºäº `MiniGPT` å¢åŠ ä¸€ä¸ª `value_head`ï¼Œè¾“å‡ºæ ‡é‡ä»·å€¼ã€‚
2.  **Reward Function**:
    *   **Format Reward**: æ£€æŸ¥è¾“å‡ºæ˜¯å¦ç¬¦åˆæ ¼å¼ï¼ˆå¦‚åŒ…å« `<think>` æ ‡ç­¾ï¼‰ï¼Œç¬¦åˆç»™åˆ†ï¼Œä¸ç¬¦åˆæ‰£åˆ†ã€‚
    *   **Model Reward**: ä½¿ç”¨å¦ä¸€ä¸ªè®­ç»ƒå¥½çš„ Reward Model æ‰“åˆ†ã€‚
3.  **PPO Step**:
    *   è®¡ç®— `Advantage` (GAE)ã€‚
    *   è®¡ç®— `Policy Loss` (Clipping)ã€‚
    *   è®¡ç®— `Value Loss`ã€‚

## ğŸ† é˜¶æ®µåï¼šGRPO (Group Relative Policy Optimization) â€”â€” DeepSeek-R1 åŒæ¬¾ç®—æ³•

**ç›®æ ‡**ï¼šæŠ›å¼ƒ Critic æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨ç»„å†…ç›¸å¯¹å¥–åŠ±æ¥ä¼˜åŒ– Policyï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨ã€‚

### 10.1 æ ¸å¿ƒä»»åŠ¡
åˆ›å»º `trainer/train_grpo.py`ã€‚è¿™æ˜¯ DeepSeek-R1 æå‡ºçš„æ ¸å¿ƒç®—æ³•ã€‚
1.  **Group Sampling**:
    *   å¯¹äºæ¯ä¸€ä¸ª Promptï¼Œè®© Policy Model é‡‡æ ·ç”Ÿæˆ $G$ ä¸ªä¸åŒçš„ Responses (e.g., $G=4$)ã€‚
2.  **Reward Calculation**:
    *   å¯¹è¿™ $G$ ä¸ªå›å¤åˆ†åˆ«è®¡ç®— Reward (è§„åˆ™åˆ† + æ¨¡å‹åˆ†)ã€‚
    *   è®¡ç®—ç»„å†…å¹³å‡åˆ† $\mu$ å’Œæ ‡å‡†å·® $\sigma$ã€‚
    *   è®¡ç®—ä¼˜åŠ¿å‡½æ•° (Advantage): $A_i = \frac{R_i - \mu}{\sigma}$ã€‚
3.  **Optimization**:
    *   æœ€å¤§åŒ– $E[\frac{\pi(y|x)}{\pi_{old}(y|x)} \cdot A_i]$ã€‚
    *   åŒæ—¶æ·»åŠ  KL æ•£åº¦çº¦æŸï¼Œé˜²æ­¢åç¦» Reference Model å¤ªè¿œã€‚

### 10.2 GRPO vs PPO
*   **PPO**: éœ€è¦ 4 ä¸ªæ¨¡å‹ (Actor, Critic, Ref, Reward)ã€‚æ˜¾å­˜å ç”¨å·¨å¤§ã€‚
*   **GRPO**: åªéœ€è¦ 2 ä¸ªæ¨¡å‹ (Actor, Ref)ã€‚Reward å¯ä»¥æ˜¯ç®€å•çš„è§„åˆ™å‡½æ•°ï¼ˆå¦‚æ•°å­¦é¢˜åˆ¤å·ï¼‰ã€‚æ˜¾å­˜æåº¦èŠ‚çœï¼Œä¸”æ•ˆæœå¾€å¾€æ›´å¥½ã€‚

### ğŸ§ª éªŒè¯ç¯èŠ‚
è¿è¡Œ `trainer/train_grpo.py`ã€‚
è§‚å¯Ÿæ—¥å¿—ä¸­ç”Ÿæˆçš„ Responsesï¼Œä½ ä¼šå‘ç°éšç€è®­ç»ƒè¿›è¡Œï¼Œæ¨¡å‹å¼€å§‹æ›´å€¾å‘äºç”Ÿæˆå¸¦æœ‰ `<think>` æ ‡ç­¾ä¸”å¾—åˆ†æ›´é«˜çš„å›ç­”ã€‚

### ğŸ“‚ æœ€ç»ˆå®Œå…¨ä½“é¡¹ç›®ç»“æ„
```text
minigpt/
â”œâ”€â”€ dataset/         # æ•°æ®å¤„ç† (Pretrain, SFT, DPO, RLHF)
â”œâ”€â”€ model/           # æ¨¡å‹å®šä¹‰ (MiniGPT, LoRA, Critic)
â”œâ”€â”€ trainer/         # è®­ç»ƒè„šæœ¬ (Pretrain, SFT, LoRA, Reason, PPO, GRPO)
â”œâ”€â”€ scripts/         # æ¨ç† & Demo
â”œâ”€â”€ out/             # å­˜æ”¾è®­ç»ƒå¥½çš„æƒé‡
â””â”€â”€ tests/           # æµ‹è¯•è„šæœ¬
```

---

æŒ‰ç…§è¿™ä¸ªç»“æ„ä¸€æ­¥æ­¥æ¥ï¼Œæ¯ä¸€æ­¥éƒ½è¿è¡Œæµ‹è¯•ä»£ç éªŒè¯ï¼Œä½ å°†ä¸ä¼šè¿·å¤±åœ¨å¤æ‚çš„ä»£ç ä¸­ã€‚ç¥ä½  coding æ„‰å¿«ï¼
